# OpenAI Realtime API æœåŠ¡ï¼ˆç®€åŒ–ç‰ˆ - åªåšéŸ³é¢‘ I/Oï¼‰
# Simplified version - Audio I/O only (STT + TTS)

import json
import base64
import logging
from typing import Optional, AsyncGenerator, Dict, Any
import websockets
import asyncio

logger = logging.getLogger(__name__)


class RealtimeService:
    """
    OpenAI Realtime API å®¢æˆ·ç«¯ï¼ˆç®€åŒ–ç‰ˆï¼‰

    åŠŸèƒ½ï¼š
    1. è¿æ¥ OpenAI Realtime API
    2. å¤„ç†éŸ³é¢‘è¾“å…¥ï¼ˆç”¨æˆ·è¯­éŸ³ â†’ æ–‡æœ¬ STTï¼‰
    3. å¤„ç†éŸ³é¢‘è¾“å‡ºï¼ˆæ–‡æœ¬ â†’ è¯­éŸ³ TTSï¼‰
    4. ä¸šåŠ¡é€»è¾‘ç”±å¤–éƒ¨ State Machine æ§åˆ¶
    """

    def __init__(
        self,
        api_key: str,
        model: str = "gpt-realtime",
        voice: str = "marin",
        instructions: Optional[str] = None
    ):
        """
        åˆå§‹åŒ– RealtimeService

        å‚æ•°:
            api_key: OpenAI API Key
            model: Realtime API æ¨¡å‹åç§°
            voice: è¯­éŸ³ç±»å‹ï¼ˆalloy, echo, shimmer, marin ç­‰ï¼‰
            instructions: è‡ªå®šä¹‰ç³»ç»ŸæŒ‡ä»¤ï¼ˆé»˜è®¤ä¸ºç®€å•çš„ TTS æŒ‡ä»¤ï¼‰
        """
        self.api_key = api_key
        self.model = model
        self.voice = voice
        self.instructions = instructions or "You are a text-to-speech system. When you receive a message, repeat it exactly word for word. Do not add any extra content or change anything."

        # OpenAI WebSocket
        self.ws_url = f"wss://api.openai.com/v1/realtime?model={model}"
        self.ws: Optional[websockets.WebSocketClientProtocol] = None
        self.is_connected = False

    async def connect(self) -> bool:
        """
        è¿æ¥åˆ° OpenAI Realtime API

        æ­¥éª¤ï¼š
        1. å»ºç«‹ OpenAI WebSocket è¿æ¥
        2. é…ç½® sessionï¼ˆéŸ³é¢‘é…ç½® + VADï¼‰
        """
        try:
            # è¿æ¥ OpenAI Realtime API
            logger.info(f"ğŸ”— Connecting to OpenAI: {self.ws_url}")
            self.ws = await websockets.connect(
                self.ws_url,
                additional_headers={"Authorization": f"Bearer {self.api_key}"}
            )

            # é…ç½®ä¼šè¯ï¼ˆä½¿ç”¨ GA æœ€æ–°å®Œæ•´è¯­æ³•ï¼‰
            session_config = {
                "type": "session.update",        # äº‹ä»¶ç±»å‹ï¼Œå›ºå®š "session.update"
                "session": {
                    # å¿…å¡«å­—æ®µ
                    "type": "realtime",          # ä¼šè¯ç±»å‹ï¼ŒRealtime ä¸€å¾‹ "realtime"
                    "model": self.model,         # ä½¿ç”¨çš„ Realtime æ¨¡å‹åï¼ˆgpt-realtime ç­‰ï¼‰

                    # éŸ³é¢‘é…ç½®ï¼ˆå®Œæ•´çš„ GA ç»“æ„ï¼‰
                    "audio": {
                        "input": {
                            "format": {
                                "type": "audio/pcm",   # è¾“å…¥éŸ³é¢‘æ ¼å¼
                                "rate": 24000          # PCM é‡‡æ ·ç‡ï¼ˆå›ºå®š 24000ï¼‰
                            },
                            # å¯é€‰ï¼šè¾“å…¥éŸ³é¢‘è½¬å†™ï¼ˆç”¨äºè®°å½•/è°ƒè¯•ï¼‰
                            "transcription": {
                                "language": "en",              # è¾“å…¥è¯­è¨€ ISO-639-1
                                "model": "gpt-4o-transcribe"   # è½¬å†™æ¨¡å‹
                            },
                            # VAD è¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆå®Œæ•´å‚æ•°ï¼‰
                            "turn_detection": {
                                "type": "server_vad",          # æœåŠ¡å™¨ç«¯ VAD
                                "create_response": True,       # æ£€æµ‹åˆ°è¯´å®Œåè‡ªåŠ¨è§¦å‘å›å¤ â† æ”¹ä¸º True
                                #"idle_timeout_ms": 15000,      # é•¿æ—¶é—´æ²‰é»˜åå¼ºåˆ¶è®©æ¨¡å‹è¯´è¯
                                "interrupt_response": True,    # æ–°ä¸€è½®è¯´è¯æ‰“æ–­å½“å‰å›å¤
                                "prefix_padding_ms": 500,      # VAD æˆªå–å‰é¢ä¿ç•™çš„æ¯«ç§’
                                "silence_duration_ms": 800,    # åˆ¤å®š"è¯´å®Œ"çš„é™éŸ³é•¿åº¦ï¼ˆç¼©çŸ­å“åº”é€Ÿåº¦ï¼‰
                                "threshold": 0.5               # è§¦å‘é˜ˆå€¼ (0~1)ï¼Œé™ä½ä»¥æé«˜çµæ•åº¦
                            },
                            "noise_reduction": {       # è¾“å…¥é™å™ªï¼›null è¡¨ç¤ºå…³é—­
                                "type": "near_field",  # "near_field"ï¼šè€³æœº/è¿‘è®²ï¼›"far_field"ï¼šç¬”è®°æœ¬/ä¼šè®®å®¤éº¦
                            },
                        },
                        "output": {
                            "format": {
                                "type": "audio/pcm",   # è¾“å‡ºéŸ³é¢‘æ ¼å¼
                                "rate": 24000          # è¾“å‡ºé‡‡æ ·ç‡ï¼ˆå¿…å¡«ï¼‰
                            },
                            "speed": 1,              # è¯­é€Ÿï¼š0.25~1.5ï¼Œé»˜è®¤ 1.0
                            "voice": self.voice        # è¯­éŸ³ç±»å‹ï¼ˆalloy/echo/shimmer/marin ç­‰ï¼‰
                        },
                    },

                    # è¾“å‡ºæ¨¡æ€ï¼ˆéŸ³é¢‘+æ–‡æœ¬ï¼‰
                    "output_modalities": ["audio"],  # éœ€è¦æ–‡æœ¬è½¬å½•ç”¨äºè°ƒè¯•

                    # ç³»ç»ŸæŒ‡ä»¤ï¼ˆåªåš TTSï¼Œä¸åšä¸šåŠ¡é€»è¾‘ï¼‰
                    "instructions": self.instructions
                }
            }

            await self.ws.send(json.dumps(session_config))
            logger.info("âœ… Session configured (audio I/O only)")

            self.is_connected = True
            logger.info("âœ… Connected to OpenAI Realtime API")
            return True

        except Exception as e:
            logger.error(f"âŒ Failed to connect: {e}")
            self.is_connected = False
            return False

    async def send_audio(self, audio_bytes: bytes):
        """å‘é€éŸ³é¢‘æ•°æ®"""
        if not self.ws:
            raise RuntimeError("Not connected")

        audio_b64 = base64.b64encode(audio_bytes).decode('utf-8')
        await self.ws.send(json.dumps({
            "type": "input_audio_buffer.append",
            "audio": audio_b64
        }))

    async def commit_audio(self):
        """æäº¤éŸ³é¢‘ç¼“å†²åŒº"""
        if not self.ws:
            raise RuntimeError("Not connected")

        await self.ws.send(json.dumps({"type": "input_audio_buffer.commit"}))
        logger.info("ğŸ¤ Audio committed")

    async def listen_for_events(self) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ç›‘å¬ OpenAI äº‹ä»¶ï¼ˆåªå¤„ç†éŸ³é¢‘å’Œè½¬å½•äº‹ä»¶ï¼‰

        äº‹ä»¶ç±»å‹ï¼š
        - session.created: ä¼šè¯åˆ›å»º
        - conversation.item.input_audio_transcription.completed: ç”¨æˆ·è¯­éŸ³è½¬å½•å®Œæˆ
        - response.output_audio_transcript.delta: AI å›å¤æ–‡æœ¬å¢é‡
        - response.output_audio.delta: AI éŸ³é¢‘å¢é‡
        - error: é”™è¯¯äº‹ä»¶
        """
        if not self.ws:
            raise RuntimeError("Not connected")

        try:
            async for message in self.ws:
                event = json.loads(message)
                event_type = event.get("type")

                logger.debug(f"ğŸ“¥ OpenAI event: {event_type}")

                # ===== åŸºç¡€äº‹ä»¶ =====

                if event_type == "session.created":
                    yield {"type": "connection_established", "message": "Connected to AI agent"}

                elif event_type == "session.updated":
                    logger.info("âœ… Session updated successfully")

                elif event_type == "conversation.item.input_audio_transcription.completed":
                    transcript = event.get("transcript", "")
                    yield {"type": "user_transcript", "text": transcript}
                    logger.info(f"ğŸ¤ User said: {transcript}")

                # ===== éŸ³é¢‘å’Œæ–‡æœ¬è¾“å‡ºäº‹ä»¶ =====

                elif event_type == "response.output_audio_transcript.delta":
                    # AI å›å¤çš„æ–‡æœ¬è½¬å½•ï¼ˆå¢é‡ï¼‰
                    delta = event.get("delta", "")
                    yield {"type": "agent_transcript_delta", "text": delta}

                elif event_type == "response.output_audio_transcript.done":
                    # AI å›å¤çš„æ–‡æœ¬è½¬å½•ï¼ˆå®Œæˆï¼‰
                    transcript = event.get("transcript", "")
                    yield {"type": "agent_transcript_complete", "text": transcript}
                    logger.info(f"ğŸ¤– Agent said: {transcript}")

                elif event_type == "response.output_audio.delta":
                    # AI å›å¤çš„éŸ³é¢‘æ•°æ®ï¼ˆå¢é‡ï¼‰
                    audio_delta = event.get("delta", "")
                    if audio_delta:
                        logger.info(f"ğŸ”Š Audio delta received: {len(audio_delta)} chars")
                        yield {"type": "audio_delta", "audio": audio_delta}
                    else:
                        logger.warning("âš ï¸ Audio delta event but no data")

                elif event_type == "response.output_audio.done":
                    # AI å›å¤çš„éŸ³é¢‘æ•°æ®ï¼ˆå®Œæˆï¼‰
                    await asyncio.sleep(0.5)
                    yield {"type": "audio_complete"} 

                elif event_type == "response.done":
                    yield {"type": "response_complete"}
                
                # [æ–°å¢] ç›‘å¬æ‰“æ–­äº‹ä»¶ï¼Œç¡®è®¤æ˜¯å¦æ˜¯ VAD è¯¯è§¦å¯¼è‡´æœ€åä¸€å¥æ²¡å¿µå®Œ
                elif event_type == "conversation.item.truncated":
                    logger.warning("âš ï¸ AI speech truncated by user interruption (VAD triggered)")
                    yield {"type": "interruption", "message": "User interrupted AI"}
                
                # [æ–°å¢] æ ¸å¿ƒè°ƒè¯•ï¼šç›‘å¬ VAD è¯´è¯å¼€å§‹äº‹ä»¶
                # å¦‚æœçœ‹åˆ°è¿™æ¡æ—¥å¿—ï¼Œè¯´æ˜éº¦å…‹é£å¬åˆ°äº†å£°éŸ³ï¼Œå¯¼è‡´ AI é—­å˜´
                elif event_type == "input_audio_buffer.speech_started":
                    logger.warning("ğŸ”‡ VAD detected speech start (Background noise/Echo?) - AI audio stopped")
                    yield {"type": "speech_started"}

                # ===== é”™è¯¯å¤„ç† =====

                elif event_type == "error":
                    error_msg = event.get("error", {}).get("message", "Unknown error")
                    logger.error(f"âŒ OpenAI error: {error_msg}")
                    yield {"type": "error", "message": error_msg}

                # ===== æœªçŸ¥äº‹ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰=====
                else:
                    # è®°å½•æ‰€æœ‰æœªå¤„ç†çš„äº‹ä»¶ç±»å‹ï¼ˆå¸®åŠ©å‘ç° GA æ–°å¢çš„äº‹ä»¶ï¼‰
                    if event_type and not event_type.startswith("input_audio_buffer"):
                        logger.debug(f"ğŸ” Unhandled event: {event_type}")

        except Exception as e:
            logger.error(f"âŒ Error listening for events: {e}")
            yield {"type": "error", "message": str(e)}

    async def create_conversation_item(self, text: str, role: str = "assistant"):
        """
        æ‰‹åŠ¨åˆ›å»ºå¯¹è¯é¡¹ï¼ˆç”¨äº state machine æ§åˆ¶å¯¹è¯ï¼‰

        æ³¨æ„ï¼šä¸ºäº†è§¦å‘ TTSï¼Œæˆ‘ä»¬åˆ›å»º user message è€Œä¸æ˜¯ assistant messageï¼Œ
        ç„¶åè®© Realtime API æ ¹æ® instructions å¤è¿°è¿™æ¡æ¶ˆæ¯

        Args:
            text: è¦è¯´çš„æ–‡æœ¬å†…å®¹
            role: è§’è‰²ï¼ˆè¿™é‡Œä¼šè¢«å¼ºåˆ¶æ”¹ä¸º "user"ï¼‰
        """
        if not self.ws:
            raise RuntimeError("Not connected")

        # å§‹ç»ˆåˆ›å»º user messageï¼Œè®© Realtime API å¤è¿°
        await self.ws.send(json.dumps({
            "type": "conversation.item.create",
            "item": {
                "type": "message",
                "role": "system",  # å¼ºåˆ¶ä½¿ç”¨ user role
                "content": [
                    {
                        "type": "input_text",
                        "text": text
                    }
                ]
            }
        }))
        logger.info(f"ğŸ“ Created user message for TTS: {text[:50]}...")

    async def trigger_response(self):
        """
        è§¦å‘ Realtime API ç”Ÿæˆå›å¤ï¼ˆTTSï¼‰

        è¿™ä¼šè®© OpenAI å°†åˆšåˆ›å»ºçš„ assistant æ¶ˆæ¯è½¬æ¢ä¸ºè¯­éŸ³å¹¶å‘é€
        """
        if not self.ws:
            raise RuntimeError("Not connected")

        await self.ws.send(json.dumps({
            "type": "response.create"
        }))
        logger.info("ğŸ¤ Triggered response generation")

    async def disconnect(self):
        """
        æ–­å¼€è¿æ¥
        """
        # æ–­å¼€ OpenAI
        if self.ws:
            await self.ws.close()
            self.ws = None

        self.is_connected = False
        logger.info("ğŸ”Œ Disconnected from OpenAI")